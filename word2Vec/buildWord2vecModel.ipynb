{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Интро и полезная литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Литература"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [Word2vec в картинках](https://habr.com/ru/post/446530/). Статья на хабре от 2019 года.\n",
    "- [Презентация](http://www.machinelearning.ru/wiki/images/b/b3/Word2Vec.pdf) (2017 год) с примерами реализации кода на библиотеке gensim\n",
    " \n",
    "Так как Word2Vec - это уже не просто единичная реализация, а группа алгоритмов для получения векторных представлений слов, то существует масса различных пакетов на эту тему. Мы будем пользоваться gensim.\n",
    "\n",
    "- [Gensim – Руководство для начинающих](https://webdevblog.ru/gensim-rukovodstvo-dlya-nachinajushhih/). Здесь нужно изучить ряд терминов, чтобы понимать принцип работы библиотеки. Статья 2019 года.\n",
    "- [A Beginner’s Guide to Word Embedding with Gensim Word2Vec Model](https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92). Статья 2019 года. По ней хорошо учиться писать код."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Термины"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Токен** обычно означает «слово»\n",
    "- **Документ** обычно может относиться как к «предложению» так и к «абзацу»\n",
    "- **Корпус** обычно представляет собой «собрание документов» в виде пакета слов или как его еще называют мешка слов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорт локальных констант и функций"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вначале перейдём на уровень рутового пути репозитория\n",
    "(в дальнейшем всегда пишем пути от корня репозитория)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/polina/WebstormProjects/ml-bug-detector\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заимпортируем всё, что нам пригодится в дальнейшем"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shared.helpers.word2VecPreprocessing import filter_tokens_fo_Word2Vec\n",
    "from shared.consts import (\n",
    "    PATH_TO_WORD2VEC_MODEL,\n",
    "    FOLDER_WITH_JSONS, # папка, в которой лежат json c токенами\n",
    "    TOKENIZED_JSON_NAME_PREFIX # как начинается имя json, в которых лежат токенизированные скрипты\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также внешние библиотеки подключим сразу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import walk\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовим данные перед построением модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Так как токенизация была разбита на микрозадачи (чтобы хватило мощностей компьютера), то и json у нас много теперь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filenames - Список всех файлов в интересуемой нами директории\n",
    "fdirpath, dirnames, filenames = list(walk(FOLDER_WITH_JSONS))[0]\n",
    "\n",
    "tokenized_scripts_jsons_paths = list(\n",
    "    filter(\n",
    "        lambda script_name: script_name.startswith(TOKENIZED_JSON_NAME_PREFIX),\n",
    "        filenames\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно было бы пытаться объединить их все в одну, но опять же тут в зависимости от мощностей компьютера, может просто не хватить оперативки держать столько данных в памяти.\n",
    "\n",
    "Благо word2Vec может учить постепенно: вначале на одних данных, потом \"доучить\" другим набором данных.\n",
    "\n",
    "Поэтому вначале учим на первом наборе данных, а потом доучиваем оставшимися постепенно.\n",
    "\n",
    "**ВАЖНОЕ УТОЧНЕНИЕ:**\n",
    "Формируется словарь только на первом этапе обучения. Дальше он будет фиксированнным и будут лишь уточняться веса при online learning / resume learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_json_file_to_train_data(path_to_json_file):\n",
    "    tokenized_scripts_dic = {}\n",
    "    \n",
    "    tokenizedScriptsJson = open(path_to_json_file)\n",
    "    raw_tokenized_scripts_dictionary = json.load(tokenizedScriptsJson)\n",
    "\n",
    "    for fileName in raw_tokenized_scripts_dictionary.keys():\n",
    "        tokenized_scripts_dic[fileName] = filter_tokens_fo_Word2Vec(raw_tokenized_scripts_dictionary[fileName])\n",
    "        \n",
    "    return list(tokenized_scripts_dic.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3d5b719e18c4a81a972f84430d15493",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "NUMBER_JSON_FILES_TO_FORM_VOCABULARY = 3\n",
    "tokenized_scripts_list_of_lists = []\n",
    "\n",
    "for json_index in tqdm(range(NUMBER_JSON_FILES_TO_FORM_VOCABULARY)):\n",
    "    train_json_path = f\"{FOLDER_WITH_JSONS}/{tokenized_scripts_jsons_paths[0]}\"\n",
    "\n",
    "    tokenized_scripts_list_of_lists = [\n",
    "        *tokenized_scripts_list_of_lists,\n",
    "        *transform_json_file_to_train_data(train_json_path)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1500"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_scripts_list_of_lists)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь бы нужно построить модель и настроить гиперпараметры.\n",
    "\n",
    "[Эта статья](https://habr.com/ru/post/446530/) говорит, что embedding_size устанавливают обычно на 300."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2VecModel = Word2Vec(\n",
    "    # list of lists. Например: [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "    sentences=tokenized_scripts_list_of_lists,\n",
    "    \n",
    "    # The number of dimensions of the embeddings and the default is 100\n",
    "    size=300, \n",
    "    \n",
    "    #The maximum distance between a target word and words around the target word. The default window is 5.\n",
    "    window=10,\n",
    "    \n",
    "    # финальный размер словаря\n",
    "    max_final_vocab=50000, \n",
    "    \n",
    "    # игнорировать слова с частотностью ниже, чем эта (регулируется автоматически, если установлено max_final_vocab)\n",
    "    min_count = 5, \n",
    "    \n",
    "    workers=cpu_count(),\n",
    "    \n",
    "    # The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW\n",
    "    sg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Смотрим, какие наиболее близкие вектора будут к функции с названием **setTimeout** (это встроенная функция в js, которая на вход принимает функцию первым аргументом, которую нужно исполнить через заданное кол-во времени в миллисекундах, которые будут переданы вторым аргументом).\n",
    "\n",
    "Число напротив каждого слова - процент близости векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('clearTimeout', 0.9035370349884033),\n",
       " ('timeout', 0.8836937546730042),\n",
       " ('off', 0.8403403759002686),\n",
       " ('wait', 0.8383508920669556),\n",
       " ('cancel', 0.823870062828064),\n",
       " ('errorTimer', 0.8155932426452637),\n",
       " ('close', 0.81069415807724),\n",
       " ('_self', 0.7997608184814453),\n",
       " ('play', 0.7961515784263611),\n",
       " ('open', 0.7959170937538147),\n",
       " ('render', 0.7932193279266357),\n",
       " ('once', 0.7924019694328308),\n",
       " ('debounce', 0.7916122674942017),\n",
       " ('disableUntilTick', 0.788800835609436),\n",
       " ('sender', 0.7858836650848389),\n",
       " ('bind', 0.7849777936935425),\n",
       " ('destroy', 0.7844083309173584),\n",
       " ('dragOverTimeout', 0.7831509113311768),\n",
       " ('$timeout', 0.7827353477478027),\n",
       " ('schedule', 0.7814319133758545)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2VecModel.wv.most_similar('setTimeout', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что модель на маленьком кусочке данных ($\\frac{1}{43}$ от всех токенов) показывает крайне странные результаты. Да, в каждом слове можно \"натянуть\" смысл, но это все будет не то."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим лучше, какие будут результаты у модели, когда она \"доучиться\" на остальных данных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resume learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "API библиотеки gensim предоставил возможность продолжать обучение модели на новых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87e1e3e6171d422e875d404570df43d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=36.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "additional_data_jsons = tokenized_scripts_jsons_paths[NUMBER_JSON_FILES_TO_FORM_VOCABULARY:]\n",
    "\n",
    "for jsonPathIndex in tqdm(range(len(additional_data_jsons))):\n",
    "    try:\n",
    "        new_data = transform_json_file_to_train_data(f\"{FOLDER_WITH_JSONS}/{additional_data_jsons[jsonPathIndex]}\")\n",
    "    except:\n",
    "        print('Не получилось расспарсить json:', additional_data_jsons[jsonPathIndex])\n",
    "        continue\n",
    "    \n",
    "    word2VecModel.build_vocab(new_data, update=True)\n",
    "    word2VecModel.train(new_data, total_examples=word2VecModel.corpus_count, epochs=word2VecModel.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('setInterval', 0.7481271028518677),\n",
       " ('clearInterval', 0.7013082504272461),\n",
       " ('clearTimeout', 0.7007315158843994),\n",
       " ('2000', 0.6274374723434448),\n",
       " ('delay', 0.6208372712135315),\n",
       " ('timerId', 0.6147360801696777),\n",
       " ('500', 0.6064625978469849),\n",
       " ('loadMoreIfNeeded', 0.591736912727356),\n",
       " ('interval', 0.5881319046020508),\n",
       " ('timer', 0.5739376544952393),\n",
       " ('5000', 0.5733336806297302),\n",
       " ('cancel', 0.5664186477661133),\n",
       " ('timeout', 0.5661569833755493),\n",
       " ('10000', 0.5637098550796509),\n",
       " ('resize', 0.554633378982544),\n",
       " ('startTimer', 0.5520493984222412),\n",
       " ('focus', 0.5508633852005005),\n",
       " ('changeTimer', 0.5498732328414917),\n",
       " ('prepareSelectAllHack', 0.5445976257324219),\n",
       " ('2500', 0.5437654256820679)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2VecModel.wv.most_similar('setTimeout', topn=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Результаты стали гораздо более впечатляющими!\n",
    "\n",
    "- clearTimeout - обрывает запланированный вызов функции\n",
    "- timeout - через какое время исполнить функцию (один из агрументов всегда)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сохранение и загрузка модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2VecModel.save(PATH_TO_WORD2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from shared.consts import PATH_TO_WORD2VEC_MODEL\n",
    "\n",
    "word2VecModel = Word2Vec.load(PATH_TO_WORD2VEC_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('setInterval', 0.7481271028518677),\n",
       " ('clearInterval', 0.7013082504272461),\n",
       " ('clearTimeout', 0.7007315158843994),\n",
       " ('2000', 0.6274374723434448),\n",
       " ('delay', 0.6208372712135315),\n",
       " ('timerId', 0.6147360801696777),\n",
       " ('500', 0.6064625978469849),\n",
       " ('loadMoreIfNeeded', 0.591736912727356),\n",
       " ('interval', 0.5881319046020508),\n",
       " ('timer', 0.5739376544952393),\n",
       " ('5000', 0.5733336806297302),\n",
       " ('cancel', 0.5664186477661133),\n",
       " ('timeout', 0.5661569833755493),\n",
       " ('10000', 0.5637098550796509),\n",
       " ('resize', 0.554633378982544),\n",
       " ('startTimer', 0.5520493984222412),\n",
       " ('focus', 0.5508633852005005),\n",
       " ('changeTimer', 0.5498732328414917),\n",
       " ('prepareSelectAllHack', 0.5445976257324219),\n",
       " ('2500', 0.5437654256820679)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2VecModel.wv.most_similar('setTimeout', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "В нашем словаре теперь 122570 слов\n"
     ]
    }
   ],
   "source": [
    "print('В нашем словаре теперь', len(word2VecModel.wv.vocab), 'слов')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
